{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using KDB+ for a Financial Ticking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install qPython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: qPython in /Users/lauraforerocamacho/Library/Python/3.8/lib/python/site-packages (2.0.0)\n"
     ]
    }
   ],
   "source": [
    "!python3.8 -m pip install qPython --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Before run next (Create sesion, last one to test the connection)\n",
    "\n",
    "First go to kdb folder  @It is necesary to load the csv files\n",
    "\n",
    "cd path to kdb\n",
    "\n",
    "Execute in terminal:\n",
    "\n",
    "q -p 5000\n",
    "\n",
    "h:hopen `:localhost:5000\n",
    "\n",
    "h\"2+2\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qpython import qconnection\n",
    "import pandas as pd\n",
    "import datetime \n",
    "\n",
    "def create_connection():\n",
    "    q = qconnection.QConnection(host='localhost', port=5000, pandas = False)\n",
    "    # initialize connection\n",
    "    q.open()\n",
    "    \n",
    "    return q\n",
    "\n",
    "def close_sconnection(q):\n",
    "    q.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, gc\n",
    "from glob import glob\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create database and tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_database(q):\n",
    "    \n",
    "    print('IPC version: %s. Is connected: %s' % (q.protocol_version, q.is_connected()))\n",
    "\n",
    "    # Load price tick file\n",
    "    q.sendSync('tick:(\"SIDTFIFIFIS\"; enlist\"|\")0:`:../data/test/raw/test_tick_price_file.csv')\n",
    "    \n",
    "    # Create enumeration for table (this is required to create a splayed table and then a partitioned table)\n",
    "    q.sendSync('tickenum: .Q.en[`:../data/test/raw/tickprice/] tick')\n",
    "    # Save table\n",
    "    q.sendSync('rsave `tickenum')\n",
    "\n",
    "\n",
    "    # Load base tick file\n",
    "    q.sendSync('base:(\"SSSSS\"; enlist\"|\")0:`:../data/test/raw/test_tick_base_file.csv')\n",
    "    # Create enumeration for table (this is required to create a splayed table and then a partitioned table)\n",
    "    q.sendSync('baseenum: .Q.en[`:../data/test/raw/basetick/] base')\n",
    "    # Save table\n",
    "    q.sendSync('rsave `baseenum')\n",
    "\n",
    "    q.sendSync('baseenum2:get `baseenum ')\n",
    "    q.sendSync('priceenum2:get `tickenum')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def save_list_results(url, data):\n",
    "    print(\"save_list_results\")\n",
    "    df = pd.DataFrame.from_records(data)\n",
    "    df.to_csv(url)\n",
    "   \n",
    "def save_stats(url, data):\n",
    "    print(\"save_stats\")\n",
    "    df = pd.DataFrame.from_records(data)\n",
    "    df.to_csv(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: joblib in /Users/lauraforerocamacho/Library/Python/3.8/lib/python/site-packages (1.2.0)\n"
     ]
    }
   ],
   "source": [
    "!python3.8 -m pip install joblib --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "from multiprocessing.pool import Pool\n",
    "import traceback\n",
    "import time\n",
    "NUM_THREADS = 5\n",
    "NUM_POOLS = 10\n",
    "\n",
    "def load_queries(path_to_queries) -> list:\n",
    "    queries=[]\n",
    "    for file in glob(path_to_queries+'*.q'):\n",
    "        with open(file, 'r') as file:\n",
    "            data = file.read().replace('\\n', ';')\n",
    "            queries.append(data)\n",
    "    return queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_query(q,run_id, query_number, queries, path_to_save_results, data_size, print_result=False):\n",
    "    print(f\"Running query {query_number} for scale factor {data_size}, saving results at {path_to_save_results}\")\n",
    "    try:\n",
    "        start = time.time()\n",
    "        temp=np.asarray(q(queries[query_number-1], qtype=1, adjust_dtype=False))\n",
    "        df = pd.DataFrame(data=temp.tolist()).replace(False, np.NaN)\n",
    "        result=df\n",
    "        count = df.shape[0]\n",
    "        end = time.time()\n",
    "        result.to_csv(path_to_save_results.format(size=data_size, query_number=query_number))\n",
    "        stats = {\n",
    "            \"run_id\": run_id,\n",
    "            \"query_id\": query_number,\n",
    "            \"start_time\": start,\n",
    "            \"end_time\": end,\n",
    "            \"elapsed_time\": end-start,\n",
    "            \"row_count\": count,\n",
    "            'error': False\n",
    "        }\n",
    "        print(stats)\n",
    "        return stats\n",
    "    except Exception:\n",
    "        print(traceback.format_exc())\n",
    "        return {\n",
    "            \"run_id\": run_id,\n",
    "            \"query_id\": query_number,\n",
    "            \"start_time\": time.time(),\n",
    "            \"end_time\": time.time(),\n",
    "            \"elapsed_time\": 0.0,\n",
    "            \"row_count\": 0,\n",
    "            \"error\": True\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_queries(q,run_id, queries, path_to_save_results, path_to_save_stats, data_size, print_result=False):\n",
    "    stats = Parallel(n_jobs=NUM_THREADS, prefer=\"threads\")(delayed(run_query)(q,run_id, i+1, queries, path_to_save_results, data_size, print_result) for i in range(len(queries)))\n",
    "    save_list_results(path_to_save_stats, stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "def run(data_sizes,q):    \n",
    "    for i, data_size in enumerate(data_sizes):\n",
    "        queries_path = \"./queries/\"\n",
    "        result_path = \"../kdb/results/result_Q{query_number}_{size}.csv\"\n",
    "        stats_path =\"../kdb/results/test_run_stats_csv_{size}.csv\".format(size=data_size)\n",
    "        \n",
    "\n",
    "        start_create_db = time.time()\n",
    "        # Create metastore for the given size\n",
    "        create_database(q)\n",
    "        end_create_db = time.time()\n",
    "        \n",
    "        # Load queries for the given size\n",
    "        queries = load_queries(queries_path)\n",
    "\n",
    "        start_run = time.time()\n",
    "        run_queries(q, i+1, queries, result_path, stats_path, data_size)\n",
    "        end_run = time.time()\n",
    "        \n",
    "        df = pd.read_csv(stats_path)   \n",
    "        response_t= math.sqrt(df[['elapsed_time']].prod().tolist()[0])\n",
    "\n",
    "        # Saving the overall stats to csv file\n",
    "        overall_stats = [{\n",
    "            'batch_id': i+1,\n",
    "            'create_db_time': end_create_db - start_create_db,\n",
    "            'run_query_time': end_run - start_run,\n",
    "            'Response Time Metric': response_t\n",
    "            \n",
    "        }]\n",
    "\n",
    "        \n",
    "        overall_stats_path = \"../kdb/results/{size}_overall_stats.csv\".format(size=data_size)\n",
    "        save_stats(overall_stats_path, overall_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IPC version: 3. Is connected: True\n",
      "['select [10;>TradeCumulative] TradeCumulative:sum(TradeSize) by Id from priceenum2 where TradeDate=2022.11.03', 'a: select TimeStamp:last TimeStamp where not null TradePrice, TradePrice: last TradePrice where not null TradePrice by Id from priceenum2 where TradeDate=2022.11.03;b: select TimeStamp_final: last TimeStamp where not null TradePrice, TradePrice_final:last TradePrice where not null TradePrice by Id  from priceenum2 where TradeDate=2022.11.04;c: a^b;select [10;>(TradePrice_final-TradePrice)%TradePrice] Id, loss:(TradePrice_final-TradePrice)%TradePrice from c', 'a: select  last AskPrice where not null AskPrice, last BidPrice where not null BidPrice by Id from priceenum2 where TradeDate=2022.11.03;;select [10;>2*(AskPrice-BidPrice)%(AskPrice+BidPrice)] Id, Percentage_spread: 2*(AskPrice-BidPrice)%(AskPrice+BidPrice) from a', 'complete: ej[`Id;baseenum2;priceenum2];;select [1;>TradeCumulative] TradeCumulative:count 1 by Id from complete where SIC like \"COMPUTERS\"']\n",
      "Running query 1 for scale factor , saving results at ../kdb/results/result_Q{query_number}_{size}.csv\n",
      "Running query 2 for scale factor , saving results at ../kdb/results/result_Q{query_number}_{size}.csv\n",
      "Running query 3 for scale factor , saving results at ../kdb/results/result_Q{query_number}_{size}.csv\n",
      "Running query 4 for scale factor , saving results at ../kdb/results/result_Q{query_number}_{size}.csv\n",
      "{'run_id': 1, 'query_id': 3, 'start_time': 1669026511.374762, 'end_time': 1669026511.454622, 'elapsed_time': 0.07985997200012207, 'row_count': 5, 'error': False}\n",
      "{'run_id': 1, 'query_id': 2, 'start_time': 1669026511.374541, 'end_time': 1669026511.4532402, 'elapsed_time': 0.07869911193847656, 'row_count': 5, 'error': False}\n",
      "{'run_id': 1, 'query_id': 4, 'start_time': 1669026511.375011, 'end_time': 1669026511.4784498, 'elapsed_time': 0.1034388542175293, 'row_count': 1, 'error': False}\n",
      "{'run_id': 1, 'query_id': 1, 'start_time': 1669026511.373708, 'end_time': 1669026511.4811552, 'elapsed_time': 0.10744714736938477, 'row_count': 5, 'error': False}\n",
      "save_list_results\n",
      "save_stats\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lauraforerocamacho/Library/Python/3.8/lib/python/site-packages/pandas/core/missing.py:95: FutureWarning: elementwise == comparison failed and returning scalar instead; this will raise an error or perform elementwise comparison in the future.\n",
      "  new_mask = arr == x\n"
     ]
    }
   ],
   "source": [
    "#data_sizes=['10', '100','1000']\n",
    "q=create_connection()\n",
    "run(data_sizes=[''],q=q)\n",
    "close_sconnection(q)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
